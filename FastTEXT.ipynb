{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+HotfGVhXijWNCbGFj74t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityapri/NLP---FastText-Word-Representation/blob/main/FastTEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "0Uux7Qil80DX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rNQIn6nD59om"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS_PATH = \"/content/wmt-news-crawl-hi.txt\""
      ],
      "metadata": {
        "id": "0b0-IiBc85UX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regx_pattern = r\"[\\u0900-\\u0963\\u0966-\\u097F]+|\\d+\"\n",
        "\n",
        "\n",
        "token_pattern = re.compile(regx_pattern,\n",
        "                           flags = re.UNICODE)"
      ],
      "metadata": {
        "id": "LhTx7AOw-wEW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(sent):\n",
        "  return token_pattern.findall(sent)\n",
        "\n",
        "\n",
        "def load_corpus(corpus_path):\n",
        "\n",
        "  sentences = []\n",
        "\n",
        "  with open(corpus_path, \"r\",encoding='utf-8') as file:\n",
        "    for raw_line in file:\n",
        "      cleaned_line = raw_line.strip()\n",
        "      if cleaned_line == \"\":\n",
        "        continue\n",
        "      token_list = tokenize(cleaned_line)\n",
        "      sentences.append(token_list)\n",
        "\n",
        "\n",
        "  return sentences\n"
      ],
      "metadata": {
        "id": "mXwG8dhH-3yv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Load Corpus\n",
        "sentences = load_corpus(CORPUS_PATH)\n",
        "\n",
        "print(\"Total sentences read:\", len(sentences))\n",
        "print(\"Sample tokens from first sentence:\", sentences[:50][:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVHIVQ4iCCia",
        "outputId": "14e4179c-ef36-47bd-9b6e-af620d68af32"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences read: 6714\n",
            "Sample tokens from first sentence: [['बरेली', 'जोन', 'के', 'एडीजी', 'रमित', 'शर्मा', 'खुद', 'पूरे', 'मामले', 'की', 'निगरानी', 'कर', 'रहे', 'हैं'], ['बता', 'दें', 'कि', 'सीएल', 'गुप्ता', 'एक्सपोर्ट्स', 'की', 'गिनती', 'मुरादाबाद', 'के', 'दिग्गज', 'पीतल', 'कारोबारियों', 'में', 'होती', 'है'], ['किस', 'देवी', 'की', 'पूजा', 'से', 'क्या', 'लाभ', 'मिलते', 'हैं', 'एक', 'साल', 'में', 'कितनी', 'बार', 'आती', 'हैं', 'नवरात्रियां', 'देवी', 'पूजा', 'के', 'साथ', 'व्रत', 'उपवास', 'क्यों', 'करना', 'चाहिए'], ['थाना', 'रसूलाबाद', 'पुलिस', 'ने', 'नाबालिग', 'किशोरी', 'के', 'साथ', 'रेप', 'करने', 'वाले', 'एक', 'अभियुक्त', 'को', 'गिरफ्तार', 'किया', 'है'], ['हमने', 'विभिन्न', 'मंचों', 'पर', 'यह', 'मुद्दा', 'उठाते', 'हुए', 'इस', 'पर', 'बातचीत', 'का', 'प्रयास', 'किया'], ['उन्होंने', 'कहा', 'कि', 'झारखंड', 'में', 'चुनाव', 'को', 'देखते', 'हुए', 'भाजपा', 'नई', 'नई', 'घोषणाएं', 'कर', 'रही', 'है', 'उनसे', 'पूछा', 'जाना', 'चाहिए', 'कि', 'क्या', 'जो', 'सुविधाएं', 'झारखंड', 'में', 'देने', 'की', 'बात', 'भाजपा', 'वाले', 'कर', 'रहे', 'हैं'], ['फिजिकल', 'वर्क', 'के', 'लिए', 'स्पेस', 'दें'], ['फिलहाल', 'क्रिकेट', 'में', 'सबसे', 'ज्यादा', 'पैसा', 'भारतीय', 'बोर्ड', 'के', 'पास', 'है'], ['मिश्रा', 'का', 'कहना', 'है', 'कि', 'किसी', 'आरक्षक', 'की', 'इतनी', 'हिम्मत', 'नहीं', 'होती', 'कि', 'वो', 'मैदान', 'में', 'जाकर', 'किसी', 'कंपनी', 'से', 'भर्ती', 'को', 'लेकर', 'इतनी', 'बड़ी', 'सेटिंग', 'कर', 'सके', 'जबकि', 'उसी', 'जगह', 'टेंट', 'में', 'पुलिस', 'विभाग', 'के', 'बड़े', 'अधिकारी', 'भी', 'मौजूद', 'होते', 'हैं'], ['राज्य', 'सरकार', 'का', 'एक', 'वर्ष', 'का', 'कार्यकाल', 'पूर्ण', 'होने', 'पर', 'मंगलवार', 'को', 'जयपुर', 'में', 'प्रधानमंत्री', 'नरेन्द्र', 'मोदी', 'के', 'मुख्य', 'आतिथ्य', 'में', 'राज्य', 'स्तरीय', 'समारोह', 'आयोजित', 'हुआ'], ['कार्यक्रम', 'का', 'उद्घाटन', 'झारखंड', 'सरकार', 'के', 'महिला', 'बाल', 'विकास', 'एवं', 'सामाजिक', 'सुरक्षा', 'विभाग', 'मंत्री', 'बेबी', 'देवी', 'चंद्रपुरा', 'प्रखंड', 'प्रमुख', 'चांदनी', 'प्रवीण', 'बीडीओ', 'ईश्वर', 'दयाल', 'महतो', 'सीओ', 'नरेश', 'कुमार', 'वर्मा', 'उप', 'प्रमुख', 'रिंकी', 'कुमारी', 'पंसस', 'राजेंद्र', 'महतो', 'ने', 'संयुक्त', 'रूप', 'से', 'दीप', 'प्रज्ज्वलित', 'कर', 'किया'], ['लेकिन', 'हिमांशु', 'छेडछाड़', 'शुरू', 'कर', 'दी'], ['मायावती', 'का', 'बड़ा', 'फैसला', 'बसपा', 'नहीं', 'लड़ेगी', 'उपचुनाव'], ['कार्यक्रम', 'में', 'मुख्य', 'वक्ता', 'के', 'रूप', 'में', 'विषय', 'विशेषज्ञ', 'और', 'प्रशिक्षक', 'प्रबल', 'बाफना', 'और', 'समन्वयक', 'एमयूएन', 'अकादमी', 'संदीप', 'पंजाबी', 'मौजूद', 'रहे'], ['इसलिए', 'ऐसे', 'प्रयोग', 'होते', 'रहने', 'चाहिए', 'इससे', 'तकनीक', 'पर', 'भरोसा', 'बढ़ेगा', 'क्या', 'दिक्कतें', 'आ', 'सकती', 'हैं', 'वो', 'भी', 'पता', 'चलता', 'रहेगा'], ['टक्कर', 'की', 'आवाज', 'सुन', 'कर', 'गांव', 'के', 'घरों', 'से', 'बाहर', 'आ', 'गए'], ['जसप्रीत', 'बुमराह', 'प्लेयर', 'ऑफ', 'द', 'मैच', 'बने', 'थे'], ['उन्होंने', 'ईरान', 'उत्तर', 'कोरिया', 'या', 'सीरिया', 'के', 'नागरिकों', 'की', 'एंट्री', 'पर', 'प्रतिबंध', 'नहीं', 'लगाया', 'है'], ['जब', 'हम', 'आम', 'जनता', 'और', 'व्यापारियों', 'की', 'समस्या', 'नगर', 'निगम', 'लेकर', 'गए', 'तो', 'वहां', 'कोई', 'अधिकारी', 'बात', 'सुनने', 'तक', 'को', 'नहीं', 'मिला'], ['फिल्म', 'ने', 'बॉक्स', 'ऑफिस', 'पर', 'सफलता', 'का', 'परचम', 'लहराया', 'था'], ['इसके', 'लिए', 'पूरी', 'रूपरेखा', 'तैयार', 'की', 'गई', 'है'], ['ऐसे', 'में', 'मुंबई', 'को', 'ऑक्शन', 'में', 'काफी', 'समझदारी', 'से', 'बोली', 'लगानी', 'होगी'], ['सूत्रों', 'से', 'मिली', 'जानकारी', 'के', 'अनुसार', 'इंदौर', 'एयरपोर्ट', 'के', 'रनवे', 'को', 'मुंबई', 'एयरपोर्ट', 'के', 'रनवे', 'की', 'लंबाई', 'का', 'विस्तार', 'दिया', 'जाएगा'], ['पकड़े', 'गए', 'अपराधियों', 'ने', 'शनिवार', 'की', 'रात', 'करीब', 'एक', 'बजे', 'कपिसा', 'गांव', 'के', 'बसंत', 'कुमार', 'के', 'घर', 'में', 'घुस', 'कर', 'ल'], ['कार्यकर्ता', 'तो', 'विधायक', 'जी', 'मैं', 'अध्यक्ष', 'जी', 'और', 'सांसद', 'जी', 'की', 'फोटो', 'हटवा', 'देता', 'हूं'], ['इस', 'पत्र', 'में', 'कहा', 'गया', 'है', 'कि', 'जिला', 'निर्वाचन', 'कार्यालय', 'परिसर', 'कृषि', 'उपज', 'मंडी', 'सारंगढ़', 'में', 'विधानसभावार', 'निर्धारित', 'स्ट्रांग', 'रूम', 'में', 'पोल्ड', 'ईव्हीएम', 'को', 'सुरक्षित', 'रखे', 'हुए', 'हैं'], ['सरकारी', 'पावर', 'कंपनी', 'पर', 'बीते', 'सालों', 'में', 'करोड़ों', 'डॉलर', 'का', 'कर्ज़', 'चढ़', 'गया', 'है', 'और', 'वो', 'मांग', 'के', 'मुताबिक़', 'बिजली', 'की', 'आपूर्ति', 'करने', 'में', 'नाकाम', 'रही', 'है'], ['उन्होंने', 'अपनी', 'टीम', 'के', 'साथ', 'ओवरलोड', 'ट्रक', 'को', 'रोकने', 'की', 'कोशिश', 'की', 'तो', 'ड्राइवर', 'गाड़ी', 'लेकर', 'भगाने', 'लगा'], ['बिना', 'डॉक्टर', 'को', 'दिखाए', 'ही', 'कॉम्बिफ्लेम', 'की', 'गोली', 'खा', 'लेता', 'था'], ['वे', 'ऑफिस', 'भी', 'नहीं', 'आ', 'रहे', 'हैं'], ['जहां', 'से', 'मैं', 'आया', 'हूं', 'उस', 'लड़के', 'को', 'इतना', 'बड़ा', 'सम्मान', 'मिला', 'है', 'मैं', 'सोच', 'भी', 'नहीं', 'सकता'], ['फिलहाल', 'लोगों', 'की', 'स्थिति', 'सुधर', 'रही', 'है'], ['डॉक्टरों', 'ने', 'सलाह', 'दी', 'कि', 'ट्यूमर', 'को', 'निकालने', 'के', 'लिए', 'सर्जरी', 'करनी', 'पड़ेगी'], ['बेरीसिया', 'इलाके', 'में', 'तीन', 'दोस्त', 'तालाब', 'के', 'पानी', 'में', 'डूब', 'गए'], ['विपक्ष', 'सिर्फ', 'अफवाह', 'उड़ाना', 'और', 'डराता', 'रहा', 'है'], ['उसके', 'पास', 'सीरीज', 'जीतना', 'का', 'कोई', 'चांस', 'नहीं', 'है'], ['लेकिन', 'परिस्थितियां', 'बदलीं', 'तो', 'छुट्टा', 'सांड़', 'और', 'गाय', 'ने', 'यह', 'प्राकृतिक', 'चक्र', 'भी', 'उलट', 'दिया', 'है'], ['पहली', 'बार', 'परीक्षा', 'केंद्रों', 'के', 'आस', 'पास', 'की', 'फोटोकॉपी', 'की', 'दुकानें', 'बंद', 'किए', 'जाने', 'का', 'निर्णय', 'भी', 'लिया', 'गया', 'है'], ['डीएनए', 'से', 'आग', 'की', 'पहचान', 'प्रशासन', 'का', 'कहना', 'है', 'कि', 'मरने', 'वालों', 'की', 'संख्या', 'बढ़', 'सकती', 'है'], ['परिजनों', 'ने', 'कहा', 'है', 'कि', 'मृतक', 'की', 'न', 'ही', 'कोई', 'दुश्मनी', 'और', 'न', 'ही', 'किसी', 'से', 'लेना', 'देना', 'था'], ['क्लब', 'के', 'सदस्यों', 'ने', 'धार्मिक', 'परंपराओं', 'का', 'हवाला', 'देते', 'हुए', 'कहा', 'कि', 'सनातनी', 'हिंदू', 'मान्यता', 'के', 'अनुसार', 'माता', 'रानी', 'की', 'स्थापना', 'एक', 'ही', 'स्थान', 'पर', 'कम', 'से', 'कम', 'तीन', 'वर्षों', 'तक', 'की', 'जानी', 'चाहिए'], ['जिस', 'मामले', 'में', 'भभुआ', 'डीएसपी', 'ने', 'जब', 'जांच', 'किया', 'तो', 'सारे', 'मामले', 'सही', 'पाए', 'गए'], ['जब', 'बैंक', 'में', 'ई', 'रिक्शा', 'से', 'क्वेश्चन', 'पेपर', 'भेजे', 'तो', 'बैंक', 'कस्टोडियन', 'को', 'इसकी', 'शिकायत', 'करनी', 'चाहिए', 'थी'], ['नवरात्र', 'के', 'दौरान', 'यहां', 'बड़ी', 'संख्या', 'में', 'श्रद्धालु', 'आते', 'हैं'], ['जानिए', 'कैसे', 'भेजा', 'गया', 'नाम'], ['इस', 'मौके', 'पर', 'जहानाबाद', 'सांसद', 'सुरेंद्र', 'यादव', 'ने', 'कहा', 'कि', 'स्व', 'देववली', 'सिंह', 'यादव', 'नौकरी', 'करने', 'के', 'साथ', 'साथ', 'सामाजिक', 'कार्यों', 'में', 'हमेशा', 'रुचि', 'रखते', 'थे', 'और', 'समाज', 'को', 'आगे', 'ले', 'जाने', 'के', 'लिए', 'प्रयासरत', 'रहते', 'थे'], ['उन्होंने', 'आरोप', 'लगाया', 'कि', 'पुलिस', 'ने', 'पार्टी', 'के', 'कार्यकर्ता', 'का', 'गमछा', 'और', 'मोबाइल', 'भी', 'ले', 'लिया'], ['माता', 'की', 'मूर्तियां', 'अलग', 'अलग', 'रूपों', 'में', 'हैं'], ['चुनाव', 'प्रचार', 'में', 'लालू', 'प्रसाद', 'यादव', 'की', 'बेटी', 'भी', 'मैदान', 'में', 'उतरी', 'थीं'], ['बता', 'दें', 'कि', 'अभी', 'एनजीओ', 'और', 'अस्पताल', 'के', 'साथ', 'अनुबंध', 'नहीं', 'हुआ', 'है']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vocabulary Construction**"
      ],
      "metadata": {
        "id": "yekBapn5InYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(tokenized_corpus):\n",
        "\n",
        "  word_frequency_map = defaultdict(int)\n",
        "  token_count = 0\n",
        "\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "      word_frequency_map[token] += 1\n",
        "      token_count += 1\n",
        "\n",
        "  vocab = sorted(word_frequency_map)\n",
        "  token_to_index = {tok: i for i, tok in enumerate(vocab)}\n",
        "  index_to_token = vocab\n",
        "\n",
        "  return token_to_index, index_to_token, word_frequency_map, token_count"
      ],
      "metadata": {
        "id": "ol2S5eqiIxz0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_index, index_token, freq_dict, token_total = create_vocabulary(sentences)\n",
        "\n",
        "print(\"Unique vocabulary size:\", len(token_index))\n",
        "print(\"Token count in corpus:\", token_total)\n",
        "\n",
        "\n",
        "print(\"\\n50 Most Frequent Words: \")\n",
        "most_common = sorted(\n",
        "    freq_dict.items(),\n",
        "    key=lambda pair : pair[1],\n",
        "    reverse = True\n",
        ")[:50]\n",
        "\n",
        "for token, occurrences in most_common:\n",
        "    print(f\"{token} : {occurrences}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FmvwZV3NccS",
        "outputId": "65057087-49f5-4722-eafb-3917aab3366c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique vocabulary size: 13948\n",
            "Token count in corpus: 103169\n",
            "\n",
            "50 Most Frequent Words: \n",
            "के : 4268\n",
            "में : 3148\n",
            "की : 2681\n",
            "है : 2597\n",
            "को : 2007\n",
            "से : 1915\n",
            "ने : 1639\n",
            "का : 1457\n",
            "और : 1416\n",
            "पर : 1289\n",
            "कि : 1071\n",
            "हैं : 927\n",
            "भी : 839\n",
            "कर : 626\n",
            "नहीं : 622\n",
            "लिए : 613\n",
            "एक : 597\n",
            "इस : 581\n",
            "किया : 571\n",
            "गया : 555\n",
            "था : 489\n",
            "ही : 479\n",
            "बाद : 458\n",
            "हो : 452\n",
            "साथ : 428\n",
            "करने : 421\n",
            "कहा : 421\n",
            "पुलिस : 404\n",
            "गई : 389\n",
            "रहे : 336\n",
            "दिया : 331\n",
            "तो : 317\n",
            "रहा : 307\n",
            "यह : 306\n",
            "रही : 304\n",
            "बताया : 299\n",
            "थी : 286\n",
            "हुए : 279\n",
            "जा : 265\n",
            "उन्होंने : 260\n",
            "लेकर : 247\n",
            "दी : 244\n",
            "थे : 243\n",
            "अपने : 233\n",
            "गए : 229\n",
            "इसके : 228\n",
            "सिंह : 222\n",
            "लेकिन : 217\n",
            "होने : 201\n",
            "वह : 196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subsampling**"
      ],
      "metadata": {
        "id": "TbWXglHQJWzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 1e-5\n",
        "\n",
        "\n",
        "def normalize_counts(freq_map, token_count):\n",
        "\n",
        "    prob_map = {}\n",
        "    inv_total = 1.0 / token_count\n",
        "\n",
        "    for term in freq_map:\n",
        "        prob_map[term] = freq_map[term] * inv_total\n",
        "\n",
        "    return prob_map\n",
        "\n",
        "\n",
        "def accept_token(token, prob_map, threshold):\n",
        "\n",
        "    freq = prob_map[token]\n",
        "\n",
        "    score = math.sqrt(threshold / freq) + (threshold / freq)\n",
        "    if score > 1.0:\n",
        "        score = 1.0\n",
        "\n",
        "    return random.random() <= score\n",
        "\n",
        "\n",
        "def apply_subsampling(data, freq_map, token_count):\n",
        "\n",
        "    probability_table = normalize_counts(freq_map, token_count)\n",
        "\n",
        "    filtered_data = []\n",
        "\n",
        "    for sent in data:\n",
        "        retained = []\n",
        "\n",
        "        for tok in sent:\n",
        "            if accept_token(tok, probability_table, THRESHOLD):\n",
        "                retained.append(tok)\n",
        "\n",
        "        if len(retained) > 0:\n",
        "            filtered_data.append(retained)\n",
        "\n",
        "    return filtered_data\n"
      ],
      "metadata": {
        "id": "myi_lxiBI3Z4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "reduced_data = apply_subsampling(sentences, freq_dict, token_total)\n",
        "\n",
        "print(\"* Corpus Before Subsampling :\")\n",
        "for idx, sent in enumerate(sentences):\n",
        "    if idx == 20:\n",
        "        break\n",
        "    print(sent)\n",
        "\n",
        "print(\"\\n* Corpus After Subsampling :\")\n",
        "for idx, sent in enumerate(reduced_data):\n",
        "    if idx == 20:\n",
        "        break\n",
        "    print(sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0g3KfzsJgGb",
        "outputId": "eb3588c4-baba-4cb2-b0c7-11e687360e9b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Corpus Before Subsampling :\n",
            "['बरेली', 'जोन', 'के', 'एडीजी', 'रमित', 'शर्मा', 'खुद', 'पूरे', 'मामले', 'की', 'निगरानी', 'कर', 'रहे', 'हैं']\n",
            "['बता', 'दें', 'कि', 'सीएल', 'गुप्ता', 'एक्सपोर्ट्स', 'की', 'गिनती', 'मुरादाबाद', 'के', 'दिग्गज', 'पीतल', 'कारोबारियों', 'में', 'होती', 'है']\n",
            "['किस', 'देवी', 'की', 'पूजा', 'से', 'क्या', 'लाभ', 'मिलते', 'हैं', 'एक', 'साल', 'में', 'कितनी', 'बार', 'आती', 'हैं', 'नवरात्रियां', 'देवी', 'पूजा', 'के', 'साथ', 'व्रत', 'उपवास', 'क्यों', 'करना', 'चाहिए']\n",
            "['थाना', 'रसूलाबाद', 'पुलिस', 'ने', 'नाबालिग', 'किशोरी', 'के', 'साथ', 'रेप', 'करने', 'वाले', 'एक', 'अभियुक्त', 'को', 'गिरफ्तार', 'किया', 'है']\n",
            "['हमने', 'विभिन्न', 'मंचों', 'पर', 'यह', 'मुद्दा', 'उठाते', 'हुए', 'इस', 'पर', 'बातचीत', 'का', 'प्रयास', 'किया']\n",
            "['उन्होंने', 'कहा', 'कि', 'झारखंड', 'में', 'चुनाव', 'को', 'देखते', 'हुए', 'भाजपा', 'नई', 'नई', 'घोषणाएं', 'कर', 'रही', 'है', 'उनसे', 'पूछा', 'जाना', 'चाहिए', 'कि', 'क्या', 'जो', 'सुविधाएं', 'झारखंड', 'में', 'देने', 'की', 'बात', 'भाजपा', 'वाले', 'कर', 'रहे', 'हैं']\n",
            "['फिजिकल', 'वर्क', 'के', 'लिए', 'स्पेस', 'दें']\n",
            "['फिलहाल', 'क्रिकेट', 'में', 'सबसे', 'ज्यादा', 'पैसा', 'भारतीय', 'बोर्ड', 'के', 'पास', 'है']\n",
            "['मिश्रा', 'का', 'कहना', 'है', 'कि', 'किसी', 'आरक्षक', 'की', 'इतनी', 'हिम्मत', 'नहीं', 'होती', 'कि', 'वो', 'मैदान', 'में', 'जाकर', 'किसी', 'कंपनी', 'से', 'भर्ती', 'को', 'लेकर', 'इतनी', 'बड़ी', 'सेटिंग', 'कर', 'सके', 'जबकि', 'उसी', 'जगह', 'टेंट', 'में', 'पुलिस', 'विभाग', 'के', 'बड़े', 'अधिकारी', 'भी', 'मौजूद', 'होते', 'हैं']\n",
            "['राज्य', 'सरकार', 'का', 'एक', 'वर्ष', 'का', 'कार्यकाल', 'पूर्ण', 'होने', 'पर', 'मंगलवार', 'को', 'जयपुर', 'में', 'प्रधानमंत्री', 'नरेन्द्र', 'मोदी', 'के', 'मुख्य', 'आतिथ्य', 'में', 'राज्य', 'स्तरीय', 'समारोह', 'आयोजित', 'हुआ']\n",
            "['कार्यक्रम', 'का', 'उद्घाटन', 'झारखंड', 'सरकार', 'के', 'महिला', 'बाल', 'विकास', 'एवं', 'सामाजिक', 'सुरक्षा', 'विभाग', 'मंत्री', 'बेबी', 'देवी', 'चंद्रपुरा', 'प्रखंड', 'प्रमुख', 'चांदनी', 'प्रवीण', 'बीडीओ', 'ईश्वर', 'दयाल', 'महतो', 'सीओ', 'नरेश', 'कुमार', 'वर्मा', 'उप', 'प्रमुख', 'रिंकी', 'कुमारी', 'पंसस', 'राजेंद्र', 'महतो', 'ने', 'संयुक्त', 'रूप', 'से', 'दीप', 'प्रज्ज्वलित', 'कर', 'किया']\n",
            "['लेकिन', 'हिमांशु', 'छेडछाड़', 'शुरू', 'कर', 'दी']\n",
            "['मायावती', 'का', 'बड़ा', 'फैसला', 'बसपा', 'नहीं', 'लड़ेगी', 'उपचुनाव']\n",
            "['कार्यक्रम', 'में', 'मुख्य', 'वक्ता', 'के', 'रूप', 'में', 'विषय', 'विशेषज्ञ', 'और', 'प्रशिक्षक', 'प्रबल', 'बाफना', 'और', 'समन्वयक', 'एमयूएन', 'अकादमी', 'संदीप', 'पंजाबी', 'मौजूद', 'रहे']\n",
            "['इसलिए', 'ऐसे', 'प्रयोग', 'होते', 'रहने', 'चाहिए', 'इससे', 'तकनीक', 'पर', 'भरोसा', 'बढ़ेगा', 'क्या', 'दिक्कतें', 'आ', 'सकती', 'हैं', 'वो', 'भी', 'पता', 'चलता', 'रहेगा']\n",
            "['टक्कर', 'की', 'आवाज', 'सुन', 'कर', 'गांव', 'के', 'घरों', 'से', 'बाहर', 'आ', 'गए']\n",
            "['जसप्रीत', 'बुमराह', 'प्लेयर', 'ऑफ', 'द', 'मैच', 'बने', 'थे']\n",
            "['उन्होंने', 'ईरान', 'उत्तर', 'कोरिया', 'या', 'सीरिया', 'के', 'नागरिकों', 'की', 'एंट्री', 'पर', 'प्रतिबंध', 'नहीं', 'लगाया', 'है']\n",
            "['जब', 'हम', 'आम', 'जनता', 'और', 'व्यापारियों', 'की', 'समस्या', 'नगर', 'निगम', 'लेकर', 'गए', 'तो', 'वहां', 'कोई', 'अधिकारी', 'बात', 'सुनने', 'तक', 'को', 'नहीं', 'मिला']\n",
            "['फिल्म', 'ने', 'बॉक्स', 'ऑफिस', 'पर', 'सफलता', 'का', 'परचम', 'लहराया', 'था']\n",
            "\n",
            "* Corpus After Subsampling :\n",
            "['बरेली', 'जोन', 'एडीजी', 'रमित', 'पूरे', 'निगरानी', 'रहे']\n",
            "['सीएल', 'एक्सपोर्ट्स', 'गिनती', 'मुरादाबाद', 'पीतल', 'कारोबारियों']\n",
            "['कितनी', 'आती', 'नवरात्रियां', 'व्रत', 'उपवास']\n",
            "['रसूलाबाद', 'किशोरी', 'एक', 'अभियुक्त']\n",
            "['मंचों', 'मुद्दा', 'उठाते', 'प्रयास']\n",
            "['झारखंड', 'देखते', 'घोषणाएं', 'कर', 'पूछा', 'चाहिए', 'झारखंड']\n",
            "['फिजिकल', 'वर्क', 'लिए', 'स्पेस', 'दें']\n",
            "['ज्यादा', 'बोर्ड', 'पास']\n",
            "['आरक्षक', 'हिम्मत', 'जाकर', 'भर्ती', 'सेटिंग', 'सके', 'टेंट']\n",
            "['पूर्ण', 'आतिथ्य', 'समारोह']\n",
            "['उद्घाटन', 'चंद्रपुरा', 'प्रमुख', 'चांदनी', 'प्रवीण', 'बीडीओ', 'ईश्वर', 'महतो', 'सीओ', 'नरेश', 'वर्मा', 'उप', 'रिंकी', 'पंसस', 'राजेंद्र', 'महतो', 'दीप', 'प्रज्ज्वलित']\n",
            "['हिमांशु', 'छेडछाड़']\n",
            "['मायावती', 'लड़ेगी', 'उपचुनाव']\n",
            "['रूप', 'विषय', 'विशेषज्ञ', 'प्रशिक्षक', 'प्रबल', 'बाफना', 'समन्वयक', 'एमयूएन', 'अकादमी', 'संदीप', 'पंजाबी']\n",
            "['प्रयोग', 'चाहिए', 'बढ़ेगा', 'दिक्कतें', 'आ', 'चलता', 'रहेगा']\n",
            "['घरों']\n",
            "['जसप्रीत', 'प्लेयर', 'बने']\n",
            "['सीरिया', 'नागरिकों', 'प्रतिबंध']\n",
            "['हम', 'आम', 'सुनने']\n",
            "['बॉक्स', 'ऑफिस', 'सफलता', 'परचम', 'लहराया']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-Gram Generation**"
      ],
      "metadata": {
        "id": "eNQwaZh2W00B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_char_ngrams(token, low=3, high=5):\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    marked = \"<\" + token + \">\"\n",
        "    size = len(marked)\n",
        "\n",
        "\n",
        "    for n in range(low, high + 1):\n",
        "        for start in range(size - n + 1):\n",
        "            gram = marked[start:start + n]\n",
        "            results.append(gram)\n",
        "\n",
        "\n",
        "    results.append(marked)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "v6a7Q0tKW_GE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_ngram_resources(corpus, vocab_map, low=3, high=5):\n",
        "\n",
        "\n",
        "    token_to_grams = {}\n",
        "    unique_grams = set()\n",
        "\n",
        "\n",
        "    for token in vocab_map:\n",
        "        grams = extract_char_ngrams(token, low, high)\n",
        "        token_to_grams[token] = grams\n",
        "\n",
        "        for g in grams:\n",
        "            unique_grams.add(g)\n",
        "\n",
        "\n",
        "    index_to_gram = list(sorted(unique_grams))\n",
        "    gram_to_index = {}\n",
        "\n",
        "    for idx, gram in enumerate(index_to_gram):\n",
        "        gram_to_index[gram] = idx\n",
        "\n",
        "    return token_to_grams, gram_to_index, index_to_gram\n"
      ],
      "metadata": {
        "id": "c5p2Lt9uXqCH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build n-gram structures\n",
        "token_to_grams, gram_to_index, index_to_gram = prepare_ngram_resources(\n",
        "    corpus=reduced_data,\n",
        "    vocab_map=token_index\n",
        ")\n",
        "\n",
        "print(\"Vocabulary size:\", len(token_to_grams))\n",
        "print(\"Total n-grams:\", len(gram_to_index))\n",
        "\n",
        "# Let's examine a Hindi example\n",
        "sample = \"भारत\"\n",
        "print(f\"\\nCharacter n-grams for '{sample}':\")\n",
        "print(token_to_grams[sample])\n",
        "\n",
        "\n",
        "# print(gram_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnr7ZbBeXyA-",
        "outputId": "dd762843-dbe5-41c7-acec-832e2799c295"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 13948\n",
            "Total n-grams: 93506\n",
            "\n",
            "Character n-grams for 'भारत':\n",
            "['<भा', 'भार', 'ारत', 'रत>', '<भार', 'भारत', 'ारत>', '<भारत', 'भारत>', '<भारत>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip-gram + Negative Sampling (FastText)"
      ],
      "metadata": {
        "id": "TZxU0ETem4A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup & Hyperparameters"
      ],
      "metadata": {
        "id": "phqxmhsQom2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Training configuration\n",
        "VECTOR_SIZE = 100\n",
        "CTX_WINDOW = 5\n",
        "NEG_K = 5\n",
        "STEP_SIZE = 0.05\n",
        "NUM_EPOCHS = 2\n"
      ],
      "metadata": {
        "id": "CKeX7CqPm7bw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Embedding Initialization"
      ],
      "metadata": {
        "id": "fBXBMqRYouL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(token_index)\n",
        "subword_count = len(gram_to_index)\n",
        "\n",
        "# Subword (character n-gram) embeddings\n",
        "subword_matrix = np.random.uniform(\n",
        "    low=-0.5 / VECTOR_SIZE,\n",
        "    high=0.5 / VECTOR_SIZE,\n",
        "    size=(subword_count, VECTOR_SIZE)\n",
        ")\n",
        "\n",
        "# Context word embeddings\n",
        "output_matrix = np.zeros((vocab_size, VECTOR_SIZE))\n"
      ],
      "metadata": {
        "id": "fKzlM1vMowOh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Negative Sampling Table Construction"
      ],
      "metadata": {
        "id": "XoyIkNwCo41m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neg_sampling_pool(freq_dict, token_to_idx):\n",
        "    exponent = 0.75\n",
        "    pool = []\n",
        "\n",
        "    for token, count in freq_dict.items():\n",
        "        idx = token_to_idx[token]\n",
        "        repetitions = int(count ** exponent)\n",
        "        pool += [idx] * repetitions\n",
        "\n",
        "    return np.asarray(pool)\n"
      ],
      "metadata": {
        "id": "YvFvDKuDo50P"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_TABLE = create_neg_sampling_pool(freq_dict, token_index)"
      ],
      "metadata": {
        "id": "qLHSDfV4qeyr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Negative Sample Selector"
      ],
      "metadata": {
        "id": "aYODPgGqo8P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_negative_samples(num_samples):\n",
        "    return np.random.choice(NEG_TABLE, size=num_samples)\n"
      ],
      "metadata": {
        "id": "8H9-ojAJpAtk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Utility Functions"
      ],
      "metadata": {
        "id": "9B5MDU5rpCuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Activation"
      ],
      "metadata": {
        "id": "YFOUC9J3pGA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "Gvesds-QpKS8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword-Composed Center Vector"
      ],
      "metadata": {
        "id": "vD3kST_8pMuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compose_center_embedding(token):\n",
        "    gram_ids = [gram_to_index[g] for g in token_to_grams[token]]\n",
        "    combined_vector = np.sum(subword_matrix[gram_ids], axis=0)\n",
        "    return combined_vector, gram_ids\n"
      ],
      "metadata": {
        "id": "NJZOtYDZpPX0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Core Training Update (Single Pair)"
      ],
      "metadata": {
        "id": "G7wknIafpUhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_pair(focus_word, neighbor_word):\n",
        "    center_embedding, used_grams = compose_center_embedding(focus_word)\n",
        "    neighbor_idx = token_index[neighbor_word]\n",
        "\n",
        "    # Positive interaction\n",
        "    pos_score = np.dot(output_matrix[neighbor_idx], center_embedding)\n",
        "    pos_prob = logistic(pos_score)\n",
        "\n",
        "    grad_accumulator = (pos_prob - 1.0) * output_matrix[neighbor_idx]\n",
        "    output_matrix[neighbor_idx] -= STEP_SIZE * (pos_prob - 1.0) * center_embedding\n",
        "\n",
        "    # Negative interactions\n",
        "    sampled_negatives = draw_negative_samples(NEG_K)\n",
        "\n",
        "    for neg_idx in sampled_negatives:\n",
        "        neg_score = np.dot(output_matrix[neg_idx], center_embedding)\n",
        "        neg_prob = logistic(neg_score)\n",
        "\n",
        "        grad_accumulator += neg_prob * output_matrix[neg_idx]\n",
        "        output_matrix[neg_idx] -= STEP_SIZE * neg_prob * center_embedding\n",
        "\n",
        "    # Update subword embeddings\n",
        "    for gid in used_grams:\n",
        "        subword_matrix[gid] -= STEP_SIZE * grad_accumulator\n"
      ],
      "metadata": {
        "id": "-CMLXEkApY1b"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Full Training Procedure"
      ],
      "metadata": {
        "id": "mvpXXkdQpajW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training():\n",
        "    for ep in range(NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {ep + 1}/{NUM_EPOCHS}\")\n",
        "        processed = 0\n",
        "\n",
        "        for sentence in reduced_data:\n",
        "            length = len(sentence)\n",
        "\n",
        "            for idx, target in enumerate(sentence):\n",
        "                left = max(0, idx - CTX_WINDOW)\n",
        "                right = min(length, idx + CTX_WINDOW + 1)\n",
        "\n",
        "                for ctx_pos in range(left, right):\n",
        "                    if ctx_pos == idx:\n",
        "                        continue\n",
        "                    optimize_pair(target, sentence[ctx_pos])\n",
        "                    processed += 1\n",
        "\n",
        "        print(\"Total word-context updates:\", processed)\n"
      ],
      "metadata": {
        "id": "ht56sUJ-pd4m"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Training"
      ],
      "metadata": {
        "id": "FbEBPqhwrKoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLG4IM1VrK7r",
        "outputId": "f0ca3455-d942-4748-9f13-c12ba875ba8a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/2\n",
            "Total word-context updates: 143832\n",
            "\n",
            "Epoch 2/2\n",
            "Total word-context updates: 143832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Inference: Word Vector Retrieval (OOV-Safe)"
      ],
      "metadata": {
        "id": "3GGpNgOerOpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_vector(token):\n",
        "\n",
        "\n",
        "    grams = extract_char_ngrams(token)\n",
        "\n",
        "    collected = []\n",
        "    for g in grams:\n",
        "        if g in gram_to_index:\n",
        "            collected.append(subword_matrix[gram_to_index[g]])\n",
        "\n",
        "    if len(collected) == 0:\n",
        "        return None\n",
        "\n",
        "    return np.sum(collected, axis=0)\n"
      ],
      "metadata": {
        "id": "33HHgISBrURV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation"
      ],
      "metadata": {
        "id": "AWydpgghruau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(infer_vector(\"भारत\"))\n",
        "print(infer_vector(\"भारतीयता\"))   # likely, OOV but still works\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCn76xO-rv8t",
        "outputId": "8b61eaa1-e083-40fc-c105-c71ef4734d7f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.53649783  0.4138684  -0.4282412  -0.19546279 -0.08769107 -0.03322835\n",
            "  0.52304768 -0.07680185 -0.35063553 -0.39727632 -0.14661153  0.37185727\n",
            "  0.64457166 -0.19351365 -0.04703597  0.06694297 -0.43062124  0.46660899\n",
            "  0.48601888 -0.12330212 -0.00945138 -0.07981257 -0.55014665 -0.18605823\n",
            " -0.2149811   0.03957449  0.31171552 -0.5424673   0.02623764  0.02167183\n",
            "  0.16680383  0.3337147   0.33175652 -0.14946112 -0.41682449 -0.89805874\n",
            " -0.21262106 -0.25891588 -0.44789433  0.41436025  0.05022845 -0.39335527\n",
            " -0.02187473 -0.21032248 -0.22176676 -0.06459911  0.35186481 -0.50518031\n",
            "  0.0690908  -0.5589887   0.28328477 -0.34007902 -0.28606508  0.00953444\n",
            " -0.174977    0.5763924   0.33145229  0.63332627 -0.06530339 -0.20911418\n",
            " -0.04725188  0.28932217  0.44937879 -0.00732059  0.28690095  0.13439169\n",
            " -0.09598472  0.27054813 -0.11398706 -0.48597777 -0.23275836  0.46558663\n",
            " -0.22782956 -0.28766979  0.14259917  0.44851303 -0.14000117 -0.11401588\n",
            "  0.57852648  0.14240387 -0.17435748 -0.20429265 -0.08393651  0.11975731\n",
            " -0.1774862   0.02823599  0.14182537  0.24419521 -0.48242235 -0.53757794\n",
            "  0.48202138  0.08122827  0.04153418  0.09139184 -0.29533738  0.11630668\n",
            " -0.0605278   0.06393842 -0.23568193 -0.41903864]\n",
            "[-0.43654081  0.31525613 -0.32927093 -0.18960206  0.01206027 -0.06077279\n",
            "  0.18852083  0.0899236  -0.13914319 -0.27686053  0.00663806  0.25215594\n",
            "  0.43020869  0.17384614  0.15490294  0.27622695 -0.25042926  0.15063212\n",
            "  0.5480666  -0.22756202  0.29892021  0.01678049 -0.29982353 -0.28250113\n",
            " -0.00757728  0.08869671  0.30726395 -0.17997092  0.12282855  0.41994541\n",
            "  0.32249103  0.22885886  0.3209861  -0.19447545 -0.16356851 -0.62969345\n",
            " -0.31723572 -0.25010318 -0.34619151  0.41289801  0.2199507  -0.36157841\n",
            "  0.17019623 -0.16560845  0.02008219 -0.01836144  0.30858975 -0.39040691\n",
            " -0.05126979 -0.20027815  0.03906954  0.03503962  0.0026589  -0.04716091\n",
            "  0.14790665  0.42042869  0.04586977  0.4593324  -0.16618405 -0.12262634\n",
            " -0.15565605  0.18666226  0.19587104 -0.26180307  0.03069702  0.06057554\n",
            "  0.01224335  0.23532146 -0.15604428 -0.41229418 -0.07710557  0.42415964\n",
            " -0.29561603 -0.40883551 -0.13597928  0.09253455 -0.15579192 -0.18293558\n",
            "  0.43146592  0.15030291 -0.33551011  0.10301697 -0.12493311 -0.10879457\n",
            " -0.23767173  0.18475462 -0.06275243  0.30925447 -0.07298102 -0.42724217\n",
            "  0.43213221 -0.16180692 -0.02515156  0.24228972 -0.60828719 -0.00283443\n",
            "  0.30923892 -0.05767933 -0.26538572 -0.39680569]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Morphology similarity"
      ],
      "metadata": {
        "id": "qMXxvp3yr78d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = infer_vector(\"भारत\")\n",
        "v2 = infer_vector(\"भारतीयता\")\n",
        "\n",
        "cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "print(\"Cosine similarity:\", cos_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liNQmbaur996",
        "outputId": "8ecd44c3-a4a7-4454-9bf8-6c0ad9dd2751"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: 0.8090814287846109\n"
          ]
        }
      ]
    }
  ]
}